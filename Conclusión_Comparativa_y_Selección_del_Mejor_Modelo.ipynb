{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa65693b",
   "metadata": {},
   "source": [
    "# 8. Conclusión Comparativa y Selección del Mejor Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d7b35",
   "metadata": {},
   "source": [
    "Tras completar la fase de **Validación Avanzada, Tuning e Interpretabilidad** y la **Evaluación del Mejor Modelo**, se consolidó el análisis comparativo entre los tres modelos con mejor rendimiento: **KNN Avanzado**, **Random Forest Avanzado** y **XGBoost Avanzado**.\n",
    "Cada uno fue optimizado mediante **pipelines estructurados**, incorporando **búsquedas aleatorias de hiperparámetros (RandomizedSearchCV)**, validación cruzada estratificada y métodos complementarios de **interpretabilidad (Permutation y Gini Importance)**.\n",
    "\n",
    "El proceso permitió evaluar los modelos no solo en función de sus métricas predictivas, sino también considerando **su estabilidad, eficiencia computacional, capacidad explicativa y nivel de generalización**.\n",
    "A continuación, se presenta una **síntesis comparativa** con los resultados más relevantes de cada modelo.\n",
    "\n",
    "\n",
    "#### **8.1 Tabla Resumen Final de Modelos Avanzados**\n",
    "\n",
    "| Modelo                                | Métrica Principal (F1) | Tiempo Total (s) |        Balanceo        |    Optimización    |     Tuning     |       Interpretabilidad       |\n",
    "| :------------------------------------ | :--------------------: | :--------------: | :--------------------: | :----------------: | :------------: | :---------------------------: |\n",
    "| **KNN Avanzado Optimizado**           |         0.7694         |      6203.14     | Estratificado (3-fold) | RandomizedSearchCV | 25 iteraciones |     Permutation Importance    |\n",
    "| **Random Forest Avanzado Optimizado** |         0.7627         |      1736.01     | Estratificado (5-fold) | RandomizedSearchCV | 30 iteraciones | Gini + Permutation Importance |\n",
    "| **XGBoost Avanzado Optimizado**       |       **0.7711**       |    **296.81**    | Estratificado (5-fold) | RandomizedSearchCV | 25 iteraciones | Gini + Permutation Importance |\n",
    "\n",
    "\n",
    "#### **8.2 Justificación Crítica del Modelo Seleccionado como Óptimo**\n",
    "\n",
    "El análisis comparativo de los tres modelos avanzados evidencia que el **XGBoost Avanzado Optimizado** alcanzó el **mejor desempeño global** tanto en precisión como en eficiencia computacional.\n",
    "Si bien el **KNN** presentó un F1-score similar (0.7694), su costo temporal fue considerablemente mayor (más de 6 000 s), lo que lo vuelve menos viable en contextos productivos o de actualización frecuente.\n",
    "Por su parte, el **Random Forest** mostró un equilibrio adecuado entre estabilidad y capacidad explicativa, pero su F1-score (0.7627) fue ligeramente inferior y con mayor dependencia del número de árboles y profundidad.\n",
    "\n",
    "El **XGBoost** integró los beneficios de ambos enfoques —robustez del Random Forest y precisión del KNN— al combinar regularización, control de sobreajuste y aprendizaje secuencial optimizado.\n",
    "Además, su **validación cruzada de 0.7598 ± 0.0115**, junto con un tiempo de ejecución inferior a 300 s, lo posiciona como el **modelo óptimo** del proyecto.\n",
    "Las técnicas de interpretabilidad (Gini y Permutation Importance) confirmaron la coherencia financiera de las variables determinantes —**interés, deuda y retrasos en pagos**— fortaleciendo la transparencia del modelo frente a decisiones basadas en riesgo crediticio.\n",
    "\n",
    "\n",
    "#### **8.3 Discusión sobre Ventajas y Limitaciones**\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "* **XGBoost** logró la **mayor eficiencia** en entrenamiento y predicción, manteniendo alta estabilidad y precisión.\n",
    "* El uso sistemático de **RandomizedSearchCV con validación estratificada** permitió obtener configuraciones reproducibles y balanceadas.\n",
    "* La **integración de interpretabilidad dual (Gini + Permutation)** facilitó el análisis de impacto de las variables, garantizando explicaciones consistentes entre modelos.\n",
    "* La comparación metodológica evidenció una **evolución progresiva de complejidad y desempeño**, desde modelos basados en distancia (KNN) hasta ensamblados de árboles con gradiente (XGBoost).\n",
    "\n",
    "**Limitaciones:**\n",
    "\n",
    "* **KNN:** alta sensibilidad a la escala y costo computacional elevado en grandes volúmenes de datos.\n",
    "* **Random Forest:** propenso a sobreajuste si no se controla la profundidad o número de estimadores.\n",
    "* **XGBoost:** mayor complejidad en la interpretación de interacciones no lineales y dependencia del ajuste de hiperparámetros.\n",
    "\n",
    "\n",
    "En conjunto, la fase de **Validación Avanzada, Tuning e Interpretabilidad** permitió comparar modelos desde criterios técnicos y prácticos.\n",
    "El **XGBoost Avanzado Optimizado** se consolidó como el **modelo final recomendado**, al ofrecer **mayor rendimiento, eficiencia y explicabilidad**, siendo la alternativa más robusta para la **predicción del estado crediticio y la toma de decisiones basada en datos**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
